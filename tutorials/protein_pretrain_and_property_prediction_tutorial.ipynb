{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein pretraining and property prediction\n",
    "\n",
    "In this tuorial, we will go through how to run a sequence model for protein property prediction. In particular, we will demonstrate how to pretrain it and how to finetune in the downstream tasks.\n",
    "\n",
    "In recent years, with sequencing technology development, the protein sequence database scale has significantly increased. However, the cost of obtaining labeled protein sequences is still very high, as it requires biological experiments. Besides, due to the inadequate number of labeled samples, the model has a high probability of overfitting the data. Borrowing the ideas from natural language processing (NLP), we can pre-train numerous unlabeled sequences by self-supervised learning. In this way, we can extract useful biological information from proteins and transfer them to other tagged tasks to make these tasks training faster and more stable convergence. These instructions refer to the work of paper TAPE, providing the model implementation of Transformer, LSTM, and ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Pretraining / Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../apps/pretrained_protein/tape')\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading related tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from utils import *\n",
    "\n",
    "paddle.enable_static() # when paddle version >= 2.0.0rc\n",
    "\n",
    "is_distributed = False\n",
    "use_cuda = False\n",
    "thread_num = 8 # for training with cpu\n",
    "\n",
    "# Setup the execution-related parameters according to the training modes.\n",
    "exe_params = default_exe_params(is_distributed=is_distributed, use_cuda=use_cuda, thread_num=thread_num)\n",
    "exe = exe_params['exe']\n",
    "trainer_num = exe_params['trainer_num']\n",
    "trainer_id = exe_params['trainer_id']\n",
    "gpu_id = exe_params['gpu_id']\n",
    "dist_strategy = exe_params['dist_strategy'] \n",
    "places = exe_params['places']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration Settings\n",
    "\n",
    "The network is setup according to `model_config`.\n",
    "- Task-related configurations\n",
    "    - \"task\": The type of training task. Candidate task types:\n",
    "        - \"pretrain\": Leverage self-supervised learning for pretraining task，for dataset `TAPE`.\n",
    "        - \"classification\": Clasification task, for dataset `Remote Homology`.\n",
    "        - \"regression\": Regression task, for datasets `Fluroscence` and `Stability`.\n",
    "        - \"seq_classification\": Sequence classification task, for dataset `Secondary Structure`.\n",
    "    - \"class_num\": The number of class for tasks `classification` and `seq_classification`.\n",
    "    - \"label_name\": The label name in the dataset.\n",
    "- Network-related configurations\n",
    "    - \"model_type\": The network type. For each network, we need to set the corresponding network hyper-parameters. We support the following networks:\n",
    "        - \"transformer\"\n",
    "            - ”hidden_size\"\n",
    "            - \"layer_num\"\n",
    "            - \"head_num\"\n",
    "        - \"lstm\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "        - \"resnet\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "            - \"filter_size\"\n",
    "- Other configurations (See the code for more details)\n",
    "    - \"dropout_rate\"\n",
    "    - \"weight_decay\"\n",
    "    \n",
    "Following is the demo `model_config` of the task of `secondary_structure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_config = \\\n",
    "{\n",
    "    \"model_name\": \"secondary_structure\",\n",
    "\n",
    "    \"task\": \"seq_classification\",\n",
    "    \"class_num\": 3,\n",
    "    \"label_name\": \"labels3\",\n",
    "\n",
    "    \"model_type\": \"lstm\",\n",
    "    \"hidden_size\": 512,\n",
    "    \"layer_num\": 3,\n",
    "\n",
    "    \"comment\": \"The following hyper-parameters are optional.\",\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"weight_decay\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Basically we build the static graph with Paddle `Program` and `Executor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tape_model import TAPEModel # More details of the network structure are shown in tape_model.py.\n",
    "from data_gen import setup_data_loader\n",
    "from pahelix.utils.paddle_utils import load_partial_params\n",
    "\n",
    "model = TAPEModel(model_config=model_config)\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "batch_size = 32 # batch size\n",
    "train_data = './demos/secondary_structure_toy_data'\n",
    "\n",
    "# prepare train_program\n",
    "train_program = fluid.Program()\n",
    "train_startup = fluid.Program()\n",
    "with fluid.program_guard(train_program, train_startup):\n",
    "    with fluid.unique_name.guard():\n",
    "        model.forward(False)\n",
    "        model.cal_loss()\n",
    "\n",
    "        # setup the optimizer\n",
    "        optimizer = default_optimizer(lr=lr, warmup_steps=0, max_grad_norm=0.1)\n",
    "        setup_optimizer(optimizer, model, use_cuda, is_distributed)\n",
    "        optimizer.minimize(model.loss)\n",
    "        \n",
    "        # setup the data loader, which provides the training data\n",
    "        train_data_loader = setup_data_loader(\n",
    "                model,\n",
    "                model_config,\n",
    "                train_data,\n",
    "                trainer_id,\n",
    "                trainer_num,\n",
    "                places,\n",
    "                batch_size)\n",
    "        exe.run(train_startup)\n",
    "\n",
    "# init_model = \"./pretrained_model\" # we could load the pre-trained model\n",
    "# load_partial_params(exe, init_model, test_program) # load the init_model\n",
    "\n",
    "save_program = train_program\n",
    "if not is_distributed:\n",
    "    save_program = train_program\n",
    "    train_program = fluid.compiler.CompiledProgram(train_program).with_data_parallel(loss_name=model.loss.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tExample: 78011\n",
      "\tAccuracy: 0.324108\n",
      "\tExample: 144800\n",
      "\tAccuracy: 0.387528\n",
      "Epoch 1\n",
      "\tExample: 78011\n",
      "\tAccuracy: 0.459704\n",
      "\tExample: 144800\n",
      "\tAccuracy: 0.457307\n"
     ]
    }
   ],
   "source": [
    "task = model_config['task']\n",
    "train_metric = get_metric(task) # evaluation metric\n",
    "train_fetch_list = model.get_fetch_list() # information needed for prediction and evaluation\n",
    "model_dir = \"./model\" # the directory to save the model\n",
    "\n",
    "for epoch_id in range(2):\n",
    "    print('Epoch %d' % epoch_id)\n",
    "    train_metric.clear() # cleanup the evaluation metric\n",
    "    for data in train_data_loader():\n",
    "        results = exe.run(\n",
    "                program=train_program,\n",
    "                feed=data,\n",
    "                fetch_list=train_fetch_list,\n",
    "                return_numpy=False)\n",
    "        update_metric(task, train_metric, results) # update the evaluation metric\n",
    "        train_metric.show() # show the results of the metrics\n",
    "    if trainer_id == 0:\n",
    "        fluid.io.save_params(exe, '%s/epoch%d' % (model_dir, epoch_id), save_program) # save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Inference\n",
    "\n",
    "In this part, we will briefly introduce how to use the trained model to do inference on the given amino acid sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parameters from ./model/epoch0.\n",
      "[[0.3337409  0.3321114  0.3341478 ]\n",
      " [0.3341086  0.33156016 0.3343312 ]\n",
      " [0.33405548 0.3311123  0.3348322 ]\n",
      " ...\n",
      " [0.33286792 0.3316798  0.33545232]\n",
      " [0.33306706 0.3318077  0.33512524]\n",
      " [0.3333515  0.33239156 0.33425692]]\n"
     ]
    }
   ],
   "source": [
    "from pahelix.utils.paddle_utils import load_partial_params\n",
    "from pahelix.utils.protein_tools import ProteinTokenizer\n",
    "from data_gen import gen_batch_data\n",
    "\n",
    "test_data = './demos/secondary_structure_toy_data'\n",
    "\n",
    "# prepare test_program\n",
    "test_program = fluid.Program()\n",
    "test_startup = fluid.Program()\n",
    "with fluid.program_guard(test_program, test_startup):\n",
    "    with fluid.unique_name.guard():\n",
    "        model.forward(True)\n",
    "        test_data_loader = setup_data_loader(\n",
    "                model,\n",
    "                model_config,\n",
    "                test_data,\n",
    "                trainer_id,\n",
    "                trainer_num,\n",
    "                places,\n",
    "                batch_size)\n",
    "        exe.run(test_startup)\n",
    "test_metric = get_metric(task)\n",
    "\n",
    "init_model = \"./model/epoch0\" # the path of initialized model\n",
    "load_partial_params(exe, init_model, test_program) # load the init_model\n",
    "\n",
    "tokenizer = ProteinTokenizer() \n",
    "test_fetch_list = model.get_fetch_list(is_inference=True)\n",
    "\n",
    "if use_cuda:\n",
    "    place = fluid.CUDAPlace(gpu_id)\n",
    "else:\n",
    "    place = fluid.CPUPlace()\n",
    "\n",
    "examples = [\n",
    "    'MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR',\n",
    "    'CCCACAGACTCAGAGAGAACCCACCATGGTGCTGTCTCCTGACGACAAGACCAACGTCAAGGCCGCCTGGGGTAAGGTCGGCGCGCACGCTGGCGAGTATGGTGCGGAGGCCCTGGAGAGGATGTTCCTGTCCTTCCCCACCACCAAGACCTACTTCCCGCACTTCGACCTGAGCCACGGCTCTGCCCAGGTTAAGGGCCACGGCAAGAAGGTGGCCGACGCGCTGACCAACGCCGTGGCGCACGTGGACGACATGCCCAACGCGCTGTCCGCCCTGAGCGACCTGCACGCGCACAAGCTTCGGGTGGACCCGGTCAACTTCAAGCTCCTAAGCCACTGCCTGCTGGTGACCCTGGCCGCCCACCTCCCCGCCGAGTTCACCCCTGCGGTGCACGCCTCCCTGGACAAGTTCCTGGCTTCTGTGAGCACCGTGCTGACCTCCAAATACCGTTAAGCTGGAGCCTCGGTGGCCATGCTTCTTGCCCCTTTGG',\n",
    "]\n",
    "inputs = gen_batch_data(examples, tokenizer, place) # data process: 1.change amino acid sequence to token ids and generate a batch\n",
    "results = exe.run(\n",
    "    program=test_program,\n",
    "    feed=inputs,\n",
    "    fetch_list=test_fetch_list,\n",
    "    return_numpy=False)\n",
    "pred = np.array(results[0])\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-my-env",
   "language": "python",
   "name": "python-my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}