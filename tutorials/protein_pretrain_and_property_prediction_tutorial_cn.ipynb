{
 "cells": [
  {
   "source": [
    "# 蛋白质预训练和性质预测\n",
    "\n",
    "近年来，随着测序技术的发展，蛋白质序列数据库的规模显著扩大。然而，必须通过湿实验才能够获得的有标注蛋白序列的成本仍然很高。此外，由于标记样本数量不足，模型有很高的概率过拟合数据。借鉴自然语言处理（NLP）的思想，通过自监督学习可以在大量无标注的蛋白序列上进行预训练。这样，我们就可以从蛋白质序列中提取有用的生物信息，并将其迁移到其他有标注的任务中，使这些任务的训练速度更快和更稳定地收敛。本教程的内容参考了 TAPE 的工作，提供了 Transformer、LSTM 和 ResNet 的模型实现。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../apps/pretrained_protein/tape')\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载相关工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from utils import *\n",
    "\n",
    "paddle.enable_static() # paddle 版本 >= 2.0.0rc\n",
    "\n",
    "is_distributed = False\n",
    "use_cuda = False\n",
    "thread_num = 8\n",
    "\n",
    "# Setup the execution-related parameters according to the training modes.\n",
    "exe_params = default_exe_params(is_distributed=is_distributed, use_cuda=use_cuda, thread_num=thread_num)\n",
    "exe = exe_params['exe']\n",
    "trainer_num = exe_params['trainer_num']\n",
    "trainer_id = exe_params['trainer_id']\n",
    "gpu_id = exe_params['gpu_id']\n",
    "dist_strategy = exe_params['dist_strategy'] \n",
    "places = exe_params['places']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型配置\n",
    "\n",
    "模型的配置如下面的 `model_config` 所示。\n",
    "- 任务相关的配置\n",
    "    - \"task\"：训练任务的类型。可选的类型包括：\n",
    "        - \"pretrain\"：使用自监督学习的方法的预训练任务，如数据集 `TAPE`。\n",
    "        - \"classification\"：分类任务，如数据集 `Remote Homology`。\n",
    "        - \"regression\"：回归任务，如数据集 `Fluroscence` 和 `Stability`。\n",
    "        - \"seq_classification\"：序列分类任务，如数据集 `Secondary Structure`。\n",
    "    - \"class_num\"：任务 `classification` 和 `seq_classification` 中类别的数量。\n",
    "    - \"label_name\"：数据集中的标签名。\n",
    "- 模型相关的配置\n",
    "    - \"model_type\"：模型的类型。 对每个模型，我们需要指定相应的模型超参数。下面是我们支持的模型：\n",
    "        - \"transformer\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "            - \"head_num\"\n",
    "        - \"lstm\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "        - \"resnet\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "            - \"filter_size\"\n",
    "- 其他配置（更多细节请查阅代码）\n",
    "    - \"dropout_rate\"\n",
    "    - \"weight_decay\"\n",
    "    \n",
    "下面的 `model_config` 是模型配置的一个示例，任务的名称是 `secondary_structure`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_config = \\\n",
    "{\n",
    "    \"model_name\": \"secondary_structure\",\n",
    "\n",
    "    \"task\": \"seq_classification\",\n",
    "    \"class_num\": 3,\n",
    "    \"label_name\": \"labels3\",\n",
    "\n",
    "    \"model_type\": \"lstm\",\n",
    "    \"hidden_size\": 512,\n",
    "    \"layer_num\": 3,\n",
    "\n",
    "    \"comment\": \"The following hyper-parameters are optional.\",\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"weight_decay\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tape_model import TAPEModel # More details of the network structure are shown in tape_model.py.\n",
    "from data_gen import setup_data_loader\n",
    "\n",
    "model = TAPEModel(model_config=model_config)\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "batch_size = 32 # batch size\n",
    "train_data = './demos/secondary_structure_toy_data'\n",
    "\n",
    "# prepare train_program\n",
    "train_program = fluid.Program()\n",
    "train_startup = fluid.Program()\n",
    "with fluid.program_guard(train_program, train_startup):\n",
    "    with fluid.unique_name.guard():\n",
    "        model.forward(False)\n",
    "        model.cal_loss()\n",
    "\n",
    "        # setup the optimizer\n",
    "        optimizer = default_optimizer(lr=lr, warmup_steps=0, max_grad_norm=0.1)\n",
    "        setup_optimizer(optimizer, model, use_cuda, is_distributed)\n",
    "        optimizer.minimize(model.loss)\n",
    "        \n",
    "        # setup the data loader, which provides the training data\n",
    "        train_data_loader = setup_data_loader(\n",
    "                model,\n",
    "                model_config,\n",
    "                train_data,\n",
    "                trainer_id,\n",
    "                trainer_num,\n",
    "                places,\n",
    "                batch_size)\n",
    "        exe.run(train_startup)\n",
    "\n",
    "save_program = train_program\n",
    "if not is_distributed:\n",
    "    save_program = train_program\n",
    "    train_program = fluid.compiler.CompiledProgram(train_program).with_data_parallel(loss_name=model.loss.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tExample: 78011\n",
      "\tAccuracy: 0.309482\n",
      "\tExample: 144800\n",
      "\tAccuracy: 0.388985\n",
      "Epoch 1\n",
      "\tExample: 78011\n",
      "\tAccuracy: 0.522811\n",
      "\tExample: 144800\n",
      "\tAccuracy: 0.515760\n"
     ]
    }
   ],
   "source": [
    "task = model_config['task']\n",
    "train_metric = get_metric(task) # evaluation metric\n",
    "train_fetch_list = model.get_fetch_list() # information needed for prediction and evaluation\n",
    "model_dir = \"./model\" # the directory to save the model\n",
    "\n",
    "for epoch_id in range(2):\n",
    "    print('Epoch %d' % epoch_id)\n",
    "    train_metric.clear() # cleanup the evaluation metric\n",
    "    for data in train_data_loader():\n",
    "        results = exe.run(\n",
    "                program=train_program,\n",
    "                feed=data,\n",
    "                fetch_list=train_fetch_list,\n",
    "                return_numpy=False)\n",
    "        update_metric(task, train_metric, results) # update the evaluation metric\n",
    "        train_metric.show() # show the results of the metrics\n",
    "    if trainer_id == 0:\n",
    "        fluid.io.save_params(exe, '%s/epoch%d' % (model_dir, epoch_id), save_program) # save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parameters from ./model/epoch0.\n",
      "[[0.33362675 0.3326527  0.33372056]\n",
      " [0.33355793 0.3325862  0.33385593]\n",
      " [0.33341178 0.3326046  0.33398363]\n",
      " ...\n",
      " [0.33260044 0.331602   0.33579758]\n",
      " [0.33269978 0.33188707 0.33541316]\n",
      " [0.33314735 0.3324862  0.33436644]]\n"
     ]
    }
   ],
   "source": [
    "from pahelix.utils.paddle_utils import load_partial_params\n",
    "from pahelix.utils.protein_tools import ProteinTokenizer\n",
    "from data_gen import gen_batch_data\n",
    "\n",
    "test_data = './demos/secondary_structure_toy_data'\n",
    "\n",
    "# prepare test_program\n",
    "test_program = fluid.Program()\n",
    "test_startup = fluid.Program()\n",
    "with fluid.program_guard(test_program, test_startup):\n",
    "    with fluid.unique_name.guard():\n",
    "        model.forward(True)\n",
    "        test_data_loader = setup_data_loader(\n",
    "                model,\n",
    "                model_config,\n",
    "                test_data,\n",
    "                trainer_id,\n",
    "                trainer_num,\n",
    "                places,\n",
    "                batch_size)\n",
    "        exe.run(test_startup)\n",
    "test_metric = get_metric(task)\n",
    "\n",
    "init_model = \"./model/epoch0\" # the path of initialized model\n",
    "load_partial_params(exe, init_model, test_program) # load the init_model\n",
    "\n",
    "tokenizer = ProteinTokenizer() \n",
    "test_fetch_list = model.get_fetch_list(is_inference=True)\n",
    "\n",
    "if use_cuda:\n",
    "    place = fluid.CUDAPlace(gpu_id)\n",
    "else:\n",
    "    place = fluid.CPUPlace()\n",
    "\n",
    "examples = [\n",
    "    'MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR',\n",
    "    'CCCACAGACTCAGAGAGAACCCACCATGGTGCTGTCTCCTGACGACAAGACCAACGTCAAGGCCGCCTGGGGTAAGGTCGGCGCGCACGCTGGCGAGTATGGTGCGGAGGCCCTGGAGAGGATGTTCCTGTCCTTCCCCACCACCAAGACCTACTTCCCGCACTTCGACCTGAGCCACGGCTCTGCCCAGGTTAAGGGCCACGGCAAGAAGGTGGCCGACGCGCTGACCAACGCCGTGGCGCACGTGGACGACATGCCCAACGCGCTGTCCGCCCTGAGCGACCTGCACGCGCACAAGCTTCGGGTGGACCCGGTCAACTTCAAGCTCCTAAGCCACTGCCTGCTGGTGACCCTGGCCGCCCACCTCCCCGCCGAGTTCACCCCTGCGGTGCACGCCTCCCTGGACAAGTTCCTGGCTTCTGTGAGCACCGTGCTGACCTCCAAATACCGTTAAGCTGGAGCCTCGGTGGCCATGCTTCTTGCCCCTTTGG',\n",
    "]\n",
    "inputs = gen_batch_data(examples, tokenizer, place) # data process: 1.change amino acid sequence to token ids and generate a batch\n",
    "results = exe.run(\n",
    "    program=test_program,\n",
    "    feed=inputs,\n",
    "    fetch_list=test_fetch_list,\n",
    "    return_numpy=False)\n",
    "pred = np.array(results[0])\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('pahelix': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5f2af7e1664b1b431746bcf7b354b550c2de8600c44162302fbdafd8f3c94797"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}