{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-nevada",
   "metadata": {},
   "source": [
    "# generative molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-guinea",
   "metadata": {},
   "source": [
    "In this tutorial, we will go through how to train a sequence VAE model for generating molecules with the formate of SMILES sequence. In particular, we will demostrate how to train a VAE model and sample the generative molecules from a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-alexander",
   "metadata": {},
   "source": [
    "## Sequence VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-venue",
   "metadata": {},
   "source": [
    "![title](seq_VAE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-brief",
   "metadata": {},
   "source": [
    "## Part I: Train a seq-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-onion",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "banner-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "seq_VAE_path = '../apps/molecular_generation/seq_VAE/'\n",
    "sys.path.insert(0, os.getcwd() + \"/..\")\n",
    "sys.path.append(seq_VAE_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proud-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = seq_VAE_path + 'data/zinc_moses/train.csv'\n",
    "train_data = load_zinc_dataset(data_path)\n",
    "# get the toy data\n",
    "train_data = train_data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "joint-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fifteen-details",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1',\n",
       " 'CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1',\n",
       " 'Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO',\n",
       " 'Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C',\n",
       " 'CC1Oc2ccc(Cl)cc2N(CC(O)CO)C1=O',\n",
       " 'CCOC(=O)c1cncn1C1CCCc2ccccc21',\n",
       " 'COc1ccccc1OC(=O)Oc1ccccc1OC',\n",
       " 'O=C1Nc2ccc(Cl)cc2C(c2ccccc2Cl)=NC1O',\n",
       " 'CN1C(=O)C(O)N=C(c2ccccc2Cl)c2cc(Cl)ccc21',\n",
       " 'CCC(=O)c1ccc(OCC(O)CO)c(OC)c1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-testament",
   "metadata": {},
   "source": [
    "## define vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charming-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vocabuary based on dataset\n",
    "vocab = OneHotVocab.from_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-sudan",
   "metadata": {},
   "source": [
    "### Model Configuration Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-trial",
   "metadata": {},
   "source": [
    "The network is setup according to model_config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "convinced-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \\\n",
    "{\n",
    "    \"max_length\":80,     # max length of sequence\n",
    "    \"q_cell\": \"gru\",     # encoder RNN cell\n",
    "    \"q_bidir\": 1,        # if encoder is bidiretion\n",
    "    \"q_d_h\": 256,        # hidden size of encoder\n",
    "    \"q_n_layers\": 1,     # number of layers of encoder RNN\n",
    "    \"q_dropout\": 0.5,    # encoder drop out rate\n",
    "\n",
    "\n",
    "    \"d_cell\": \"gru\",     # decoder RNN cell\n",
    "    \"d_n_layers\":3,      # number of decoder layers\n",
    "    \"d_dropout\":0.2,     # decoder drop out rate\n",
    "    \"d_z\":128,           # latent space size\n",
    "    \"d_d_h\":512,         # hidden size of decoder\n",
    "    \"freeze_embeddings\":0 # if freeze embeddings\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-sympathy",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "round-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL] POLAR_ANGLE_NUM:10\n"
     ]
    }
   ],
   "source": [
    "from pahelix.model_zoo.seq_vae_model  import VAE\n",
    "# build the model\n",
    "model = VAE(vocab, model_config)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-orlando",
   "metadata": {},
   "source": [
    "### Trian the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "promotional-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training settings\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "n_epoch = 2\n",
    "kl_weight = 0.1\n",
    "\n",
    "# define optimizer\n",
    "optimizer = paddle.optimizer.Adam(parameters=model.parameters(),\n",
    "                            learning_rate=learning_rate)\n",
    "\n",
    "# build the dataset and data loader\n",
    "max_length = model_config[\"max_length\"]\n",
    "train_dataset = StringDataset(vocab, train_data, max_length)\n",
    "train_dataloader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tired-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "batch:0, kl_loss:0.334582, recon_loss:3.377764\n",
      "batch:1, kl_loss:0.235228, recon_loss:3.264235\n",
      "batch:2, kl_loss:0.195186, recon_loss:3.121852\n",
      "batch:3, kl_loss:0.193437, recon_loss:3.026339\n",
      "batch:4, kl_loss:0.200334, recon_loss:2.934668\n",
      "batch:5, kl_loss:0.212199, recon_loss:2.871092\n",
      "batch:6, kl_loss:0.221612, recon_loss:2.817064\n",
      "batch:7, kl_loss:0.227276, recon_loss:2.761942\n",
      "batch:8, kl_loss:0.229593, recon_loss:2.721181\n",
      "batch:9, kl_loss:0.227553, recon_loss:2.692791\n",
      "batch:10, kl_loss:0.222633, recon_loss:2.666499\n",
      "batch:11, kl_loss:0.215547, recon_loss:2.642660\n",
      "batch:12, kl_loss:0.206956, recon_loss:2.621526\n",
      "batch:13, kl_loss:0.197874, recon_loss:2.602398\n",
      "batch:14, kl_loss:0.188871, recon_loss:2.583047\n",
      "batch:15, kl_loss:0.180314, recon_loss:2.564440\n",
      "epoch:0 loss:2.582471 kl_loss:0.180314 recon_loss:2.564440\n",
      "#######################\n",
      "batch:0, kl_loss:0.045176, recon_loss:2.305443\n",
      "batch:1, kl_loss:0.041685, recon_loss:2.304310\n",
      "batch:2, kl_loss:0.038983, recon_loss:2.303871\n",
      "batch:3, kl_loss:0.036679, recon_loss:2.293861\n",
      "batch:4, kl_loss:0.034720, recon_loss:2.299821\n",
      "batch:5, kl_loss:0.033109, recon_loss:2.301628\n",
      "batch:6, kl_loss:0.031707, recon_loss:2.297117\n",
      "batch:7, kl_loss:0.030567, recon_loss:2.292929\n",
      "batch:8, kl_loss:0.029616, recon_loss:2.289356\n",
      "batch:9, kl_loss:0.028776, recon_loss:2.285768\n",
      "batch:10, kl_loss:0.028142, recon_loss:2.285516\n",
      "batch:11, kl_loss:0.027534, recon_loss:2.280182\n",
      "batch:12, kl_loss:0.026872, recon_loss:2.277198\n",
      "batch:13, kl_loss:0.026270, recon_loss:2.275144\n",
      "batch:14, kl_loss:0.025680, recon_loss:2.268615\n",
      "batch:15, kl_loss:0.025032, recon_loss:2.264457\n",
      "epoch:1 loss:2.266961 kl_loss:0.025032 recon_loss:2.264457\n"
     ]
    }
   ],
   "source": [
    "# start to train \n",
    "for epoch in range(n_epoch):\n",
    "    print('#######################')\n",
    "    kl_loss_values = []\n",
    "    recon_loss_values = []\n",
    "    loss_values = []\n",
    "    \n",
    "    for batch_id, data in enumerate(train_dataloader()):\n",
    "        # read batch data\n",
    "        data_batch = data\n",
    "\n",
    "        # forward\n",
    "        kl_loss, recon_loss  = model(data_batch)\n",
    "        loss = kl_weight * kl_loss + recon_loss\n",
    "\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # clear gradients\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # gathering values from each batch\n",
    "        kl_loss_values.append(kl_loss.numpy())\n",
    "        recon_loss_values.append(recon_loss.numpy())\n",
    "        loss_values.append(loss.numpy())\n",
    "\n",
    "        \n",
    "        print('batch:%s, kl_loss:%f, recon_loss:%f' % (batch_id, float(np.mean(kl_loss_values)), float(np.mean(recon_loss_values))))\n",
    "        \n",
    "    print('epoch:%d loss:%f kl_loss:%f recon_loss:%f' % (epoch, float(np.mean(loss_values)),float(np.mean(kl_loss_values)),float(np.mean(recon_loss_values))),flush=True)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-contractor",
   "metadata": {},
   "source": [
    "## Part II: Sample from prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pahelix.utils.metrics.molecular_generation.metrics_ import *\n",
    "N_samples = 1000  # number of samples \n",
    "max_len = 80      # maximum length of samples\n",
    "current_samples = model.sample(N_samples,max_len)  # get the samples from pre-trained model\n",
    "\n",
    "metrics = get_all_metrics(gen=current_samples,k=[100])  # get the evaluation from samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
